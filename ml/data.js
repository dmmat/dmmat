const examData = {
  tickets: [
    {
      id: 1,
      questions: [
        {
          question: "Функції активації в глибоких нейронних мережах",
          answer: `<h4>Функції активації в глибоких нейронних мережах</h4>
          <p>
          Функції активації є ключовим компонентом нейронних мереж, що визначають вихідні значення нейронів на основі їх вхідних сигналів. У глибоких мережах найпопулярнішою є функція ReLU (Rectified Linear Unit), яка повертає максимум між нулем та вхідним значенням, що допомагає уникнути проблеми зникнення градієнта. Функція Sigmoid застосовується рідше через насичення при великих значеннях, але корисна для вихідних шарів бінарної класифікації. Функція Tanh має аналогічну до Sigmoid форму, але центрована навколо нуля, що робить її більш підходящою для прихованих шарів. Для більш складних задач використовуються сучасні варіанти як Leaky ReLU, ELU та Swish, які мають кращі властивості градієнта. У Keras функції активації задаються параметром activation='relu' або як окремі шари.
          </p>`,
        },
        {
          question: "Регуляризація на основі Max-норми",
          answer: `<h4>Регуляризація на основі Max-норми</h4>
          <p>
          Max-норма регуляризація є технікою запобігання перенавчанню, яка обмежує максимальне значення ваг нейронної мережі. На відміну від L1 та L2 регуляризації, max-норма встановлює жорсткий ліміт на норму вектора ваг кожного нейрона, зазвичай обмежуючи її значенням від 2 до 4. Цей метод особливо ефективний у поєднанні з dropout, оскільки допомагає стабілізувати навчання після випадкового вимкнення нейронів. В Keras max-норма реалізується через constraint=keras.constraints.MaxNorm(max_value=2.0) при визначенні шарів. Основна перевага цього підходу полягає в тому, що він не додає додаткових членів до функції втрат, а просто обрізає ваги після кожного оновлення. Max-норма регуляризація демонструє особливо хороші результати в рекурентних нейронних мережах та при роботі з нестабільними градієнтами.
          </p>`,
        },
        {
          question: "Задача. Написати архітектуру простої нейронної мережі використовуючи функцію активації ReLU",
          answer: `<h4>Архітектура нейронної мережі з ReLU</h4>
          <p>
          Для створення простої нейронної мережі з функцією активації ReLU використовуємо Sequential API в Keras. Мережа складається з вхідного шару Dense з 128 нейронами та активацією ReLU, прихованого шару з 64 нейронами та тією ж активацією, і вихідного шару з кількістю нейронів відповідно до задачі класифікації. Функція ReLU застосовується до всіх прихованих шарів для забезпечення нелінійності та уникнення проблеми зникнення градієнта. Компіляція моделі включає оптимізатор Adam, функцію втрат sparse_categorical_crossentropy для багатокласової класифікації та метрику accuracy для оцінки продуктивності. Ця архітектура є базовою та може бути адаптована під конкретні задачі шляхом зміни кількості нейронів, шарів та параметрів навчання.
          </p>
          <pre><code>
import tensorflow as tf
from tensorflow import keras

model = keras.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(10, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()
          </code></pre>`,
        },
      ],
    },
    {
      id: 2,
      questions: [
        {
          question: "Різниця між класичним машинним навчанням та глибоким навчанням",
          answer: `<h4>Різниця між класичним машинним навчанням та глибоким навчанням</h4>
          <p>
          Класичне машинне навчання вимагає ручного виділення ознак (feature engineering), де експерти предметної області визначають які характеристики даних є важливими для навчання моделі. Глибоке навчання автоматично виявляє складні патерни та ієрархічні представлення даних через багатошарові нейронні мережі без необхідності попереднього виділення ознак. Класичні алгоритми як SVM, Random Forest та логістична регресія зазвичай працюють з табличними даними та потребують менше обчислювальних ресурсів. Глибокі мережі особливо ефективні для неструктурованих даних як зображення, текст та аудіо, але вимагають значних обчислювальних потужностей та великих обсягів даних. У sklearn реалізовані класичні алгоритми, тоді як TensorFlow/Keras призначені для глибокого навчання. Класичні методи часто більш інтерпретовані та швидші у навчанні на малих датасетах, проте глибоке навчання показує кращі результати на складних задачах з великими обсягами даних.
          </p>`,
        },
        {
          question: "Навчання глибоких нейронних мереж: проблема зникнення градієнта і її вирішення",
          answer: `<h4>Проблема зникнення градієнта та її вирішення</h4>
          <p>
          Проблема зникнення градієнта виникає в глибоких мережах, коли градієнти стають експоненційно малими при поширенні від вихідних до вхідних шарів під час зворотного поширення помилки. Це призводить до повільного або відсутнього навчання початкових шарів мережі, особливо критично для мереж з функціями активації sigmoid або tanh. Основними рішеннями є використання функції активації ReLU та її варіантів, які мають сталі градієнти для додатних значень. Техніки правильної ініціалізації ваг, такі як ініціалізація Glorot (Xavier) або He, допомагають підтримувати стабільні градієнти на початку навчання. Batch normalization нормалізує входи кожного шару, стабілізуючи навчання та дозволяючи використовувати вищі швидкості навчання. Residual connections (skip connections) в архітектурах типу ResNet дозволяють градієнтам протікати напряму через мережу, ефективно вирішуючи проблему зникнення градієнта в дуже глибоких мережах.
          </p>`,
        },
        {
          question: "Задача. Поясніть частину коду conv = keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\")",
          answer: `<h4>Пояснення згорткового шару Conv2D</h4>
          <p>
          Даний код створює двовимірний згортковий шар в Keras, який є основним компонентом згорткових нейронних мереж для обробки зображень. Параметр filters=32 означає, що шар містить 32 фільтри (ядра згортки), кожен з яких виявляє певні ознаки у вхідних даних та створює 32 карти ознак. Kernel_size=3 визначає розмір фільтрів як 3x3 пікселі, що є стандартним розміром для виявлення локальних патернів у зображеннях. Strides=1 означає, що фільтр зсувається на один піксель за раз по горизонталі та вертикалі під час згортки. Параметр padding="same" додає нулі по краях вхідного зображення так, щоб розмір вихідної карти ознак збігався з розміром входу. Activation="relu" застосовує функцію ReLU до результату згортки, забезпечуючи нелінійність та допомагаючи мережі вивчати складні патерни. Цей шар зазвичай використовується на початку згорткових мереж для виявлення базових ознак як краї та текстури.
          </p>`,
        },
      ],
    },
    {
      id: 3,
      questions: [
        {
          question: "Проблема зникання/вибуху градієнтів: Ініціалізація Glorot",
          answer: `<h4>Ініціалізація Glorot для вирішення проблем градієнтів</h4>
          <p>
          Ініціалізація Glorot (також відома як Xavier ініціалізація) розроблена для вирішення проблем зникання та вибуху градієнтів в глибоких нейронних мережах шляхом правильного налаштування початкових ваг. Метод базується на принципі збереження дисперсії сигналів та градієнтів при проходженні через шари мережі. Ініціалізація Glorot встановлює ваги випадковими значеннями з нормального або рівномірного розподілу з дисперсією, що залежить від кількості вхідних та вихідних з'єднань нейрона. Для нормального розподілу дисперсія дорівнює 2/(n_in + n_out), де n_in та n_out - кількість вхідних та вихідних з'єднань відповідно. У Keras ця ініціалізація доступна як kernel_initializer='glorot_uniform' або 'glorot_normal' та є стандартною для багатьох типів шарів. Glorot ініціалізація особливо ефективна з функціями активації sigmoid та tanh, забезпечуючи стабільне навчання глибоких мереж без значних коливань градієнтів.
          </p>`,
        },
        {
          question: "Переваги та недоліки різних оптимізаторів: Моментний оптимізатор, Прискорений градієнт Nesterov, AdaGrad",
          answer: `<h4>Порівняння оптимізаторів: Momentum, Nesterov, AdaGrad</h4>
          <p>
          Моментний оптимізатор (Momentum) додає "інерцію" до градієнтного спуску, накопичуючи експоненційно зважену середню попередніх градієнтів, що допомагає проходити локальні мінімуми та прискорює збіжність в напрямках з послідовними градієнтами. Прискорений градієнт Nesterov покращує Momentum, обчислюючи градієнт не в поточній точці, а в "заглянутій вперед" позиції, що забезпечує більш точне передбачення та швидшу збіжність. AdaGrad адаптує швидкість навчання індивідуально для кожного параметра на основі історії градієнтів, зменшуючи швидкість для часто оновлюваних параметрів та збільшуючи для рідкісних. Основний недолік AdaGrad полягає в монотонному зменшенні швидкості навчання, що може призвести до передчасної зупинки навчання. Momentum та Nesterov можуть "перестрибувати" оптимум через накопичену інерцію, особливо при неправильному налаштуванні гіперпараметрів. У Keras ці оптимізатори реалізовані як keras.optimizers.SGD(momentum=0.9), keras.optimizers.SGD(momentum=0.9, nesterov=True) та keras.optimizers.Adagrad() відповідно.
          </p>`,
        },
        {
          question: "Задача. Написати архітектуру нейронної мережі використовуючи ініціалізацію Glorot та оптимізатор AdaGrad",
          answer: `<h4>Нейронна мережа з ініціалізацією Glorot та оптимізатором AdaGrad</h4>
          <p>
          Створюємо нейронну мережу з явним використанням ініціалізації Glorot для всіх Dense шарів та оптимізатора AdaGrad для навчання. Ініціалізація Glorot uniform застосовується через параметр kernel_initializer, що забезпечує стабільні градієнти на початку навчання. Мережа складається з двох прихованих шарів з ReLU активацією та вихідного шару з softmax для класифікації. Оптимізатор AdaGrad налаштований з початковою швидкістю навчання 0.01, яка автоматично адаптується для кожного параметра на основі історії градієнтів. Така конфігурація особливо корисна для задач з розрідженими даними або нерівномірним розподілом ознак. Модель компілюється з функцією втрат categorical_crossentropy для багатокласової класифікації та метрикою accuracy для оцінки продуктивності. Ця архітектура демонструє практичне застосування теоретичних концепцій ініціалізації ваг та адаптивної оптимізації.
          </p>
          <pre><code>
import tensorflow as tf
from tensorflow import keras

model = keras.Sequential([
    keras.layers.Dense(128, 
                      activation='relu', 
                      kernel_initializer='glorot_uniform',
                      input_shape=(784,)),
    keras.layers.Dense(64, 
                      activation='relu',
                      kernel_initializer='glorot_uniform'),
    keras.layers.Dense(10, 
                      activation='softmax',
                      kernel_initializer='glorot_uniform')
])

optimizer = keras.optimizers.Adagrad(learning_rate=0.01)

model.compile(
    optimizer=optimizer,
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()
          </code></pre>`,
        },
      ],
    },
    {
      id: 4,
      questions: [
        {
          question: "Навчання глибоких нейронних мереж: перенесене навчання",
          answer: `<h4>Перенесене навчання в глибоких нейронних мережах</h4>
          <p>
          Перенесене навчання (Transfer Learning) є потужною технікою, яка дозволяє використовувати попередньо навчені моделі на нових задачах з обмеженою кількістю даних. Основна ідея полягає в тому, що нижні шари глибоких мереж вивчають загальні ознаки (краї, форми, текстури), які корисні для багатьох задач комп'ютерного зору. Зазвичай використовують попередньо навчені моделі на великих датасетах як ImageNet, заморожуючи ваги базової мережі та дотренировуючи лише верхні шари на цільовій задачі. Це значно скорочує час навчання та вимоги до обчислювальних ресурсів порівняно з навчанням з нуля. У Keras перенесене навчання реалізується через keras.applications, де доступні популярні архітектури як VGG, ResNet, InceptionV3 з попередньо навченими вагами. Метод особливо ефективний, коли цільовий датасет схожий на вихідний або коли доступна обмежена кількість навчальних даних, демонструючи значні покращення у точності та швидкості збіжності.
          </p>`,
        },
        {
          question: "Що таке планування швидкості навчання та як воно використовується?",
          answer: `<h4>Планування швидкості навчання</h4>
          <p>
          Планування швидкості навчання (Learning Rate Scheduling) є технікою динамічного регулювання швидкості навчання під час тренування нейронної мережі для покращення збіжності та якості навчання. Початково висока швидкість навчання дозволяє швидко наближатися до оптимуму, а поступове зменшення допомагає точніше налаштувати ваги та уникнути коливань навколо мінімуму. Популярними стратегіями є експоненційне зменшення, ступінчасте зменшення через фіксовані інтервали епох та косинусне загасання. У Keras планування реалізується через keras.callbacks.LearningRateScheduler або keras.callbacks.ReduceLROnPlateau, який автоматично зменшує швидкість при зупинці покращення метрики. Адаптивне планування на основі валідаційної втрати особливо корисне, оскільки реагує на фактичний прогрес навчання. Правильне планування швидкості навчання може значно покращити фінальну точність моделі та забезпечити більш стабільний процес навчання без ручного налаштування гіперпараметрів.
          </p>`,
        },
        {
          question: "Задача. Написати архітектуру нейронної мережі використовуючи планування швидкості навчання",
          answer: `<h4>Нейронна мережа з плануванням швидкості навчання</h4>
          <p>
          Створюємо архітектуру нейронної мережі з реалізацією планування швидкості навчання через callback функції Keras. Модель складається з трьох Dense шарів з dropout для регуляризації та ReLU активацією в прихованих шарах. Використовуємо ReduceLROnPlateau callback, який автоматично зменшує швидкість навчання в 2 рази, якщо валідаційна втрата не покращується протягом 3 епох. Цей підхід дозволяє моделі адаптуватися до складності задачі та уникати застрягання в локальних мінімумах. Початкова швидкість навчання встановлена на 0.001 в оптимізаторі Adam, а callback забезпечує динамічне регулювання під час навчання. Мінімальна швидкість навчання обмежена значенням 0.0001 для запобігання повної зупинки навчання. Така конфігурація забезпечує оптимальний баланс між швидкістю збіжності та стабільністю навчального процесу.
          </p>
          <pre><code>
import tensorflow as tf
from tensorflow import keras

model = keras.Sequential([
    keras.layers.Dense(256, activation='relu', input_shape=(784,)),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(10, activation='softmax')
])

model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

lr_scheduler = keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    min_lr=0.0001,
    verbose=1
)

# Використання при навчанні:
# model.fit(x_train, y_train, validation_data=(x_val, y_val), 
#           epochs=50, callbacks=[lr_scheduler])
          </code></pre>`,
        },
      ],
    },
    {
      id: 5,
      questions: [
        {
          question: "Використання попередньо навчених моделей",
          answer: `<h4>Використання попередньо навчених моделей</h4>
          <p>
          Попередньо навчені моделі є готовими нейронними мережами, які вже навчені на великих датасетах і можуть бути адаптовані для нових задач без навчання з нуля. У Keras доступні популярні архітектури через keras.applications, включаючи VGG16, ResNet50, InceptionV3, MobileNet та EfficientNet, навчені на ImageNet з мільйонами зображень. Основні переваги включають значне скорочення часу навчання, менші вимоги до обчислювальних ресурсів та можливість досягнення високої точності навіть з невеликими датасетами. Типовий підхід полягає в завантаженні базової моделі з weights='imagenet', заморожуванні її ваг через trainable=False, та додаванні власних класифікаційних шарів зверху. Для fine-tuning можна розморозити останні кілька шарів базової моделі і дотренувати їх з малою швидкістю навчання на специфічних даних. Цей метод особливо ефективний для задач комп'ютерного зору, обробки природної мови та інших областей, де доступні якісні попередньо навчені моделі.
          </p>`,
        },
        {
          question: "Що таке об'єднавчі (pooling) шари в згорткових нейронних мережах?",
          answer: `<h4>Об'єднавчі (pooling) шари в згорткових мережах</h4>
          <p>
          Об'єднавчі шари є важливим компонентом згорткових нейронних мереж, призначені для зменшення просторових розмірів карт ознак та зниження обчислювальної складності. Max pooling вибирає максимальне значення з кожного локального регіону карти ознак, зберігаючи найбільш виражені ознаки та забезпечуючи інваріантність до невеликих зсувів у вхідних даних. Average pooling обчислює середнє значення в кожному регіоні, що дає більш згладжене представлення ознак і може бути корисним для зменшення шуму. Зазвичай використовують розмір pooling 2x2 з stride=2, що зменшує розміри карт ознак удвічі по кожному виміру, значно скорочуючи кількість параметрів у наступних шарах. У Keras pooling шари реалізовані як keras.layers.MaxPooling2D() та keras.layers.AveragePooling2D() з можливістю налаштування розміру вікна та кроку. Global pooling варіанти як GlobalMaxPooling2D згортають всю карту ознак до одного значення, що часто використовується перед фінальними класифікаційними шарами замість Flatten.
          </p>`,
        },
        {
          question: "Задача. Написати архітектуру згорткової нейронної мережі використовуючи об'єднавчі шари",
          answer: `<h4>Архітектура згорткової мережі з pooling шарами</h4>
          <p>
          Створюємо класичну архітектуру згорткової нейронної мережі з чергуванням згорткових та pooling шарів для ефективної обробки зображень. Мережа починається з двох згорткових шарів з 32 фільтрами та ReLU активацією, після яких іде MaxPooling2D для зменшення просторових розмірів. Другий блок містить 64 фільтри з аналогічною структурою, що дозволяє виявляти більш складні ознаки на зменшених картах. Третій блок з 128 фільтрами забезпечує виявлення високорівневих патернів перед фінальною класифікацією. GlobalAveragePooling2D замінює Flatten для більш компактного представлення ознак та зменшення кількості параметрів у Dense шарі. Dropout з коефіцієнтом 0.5 запобігає перенавчанню, а фінальний Dense шар з softmax активацією виконує класифікацію. Така архітектура демонструє ефективне використання pooling для поступового зменшення просторових розмірів при збільшенні глибини ознак.
          </p>
          <pre><code>
import tensorflow as tf
from tensorflow import keras

model = keras.Sequential([
    # Перший згортковий блок
    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    keras.layers.Conv2D(32, (3, 3), activation='relu'),
    keras.layers.MaxPooling2D(pool_size=(2, 2)),
    
    # Другий згортковий блок
    keras.layers.Conv2D(64, (3, 3), activation='relu'),
    keras.layers.Conv2D(64, (3, 3), activation='relu'),
    keras.layers.MaxPooling2D(pool_size=(2, 2)),
    
    # Третій згортковий блок
    keras.layers.Conv2D(128, (3, 3), activation='relu'),
    keras.layers.Conv2D(128, (3, 3), activation='relu'),
    keras.layers.MaxPooling2D(pool_size=(2, 2)),
    
    # Класифікатор
    keras.layers.GlobalAveragePooling2D(),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(10, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()
          </code></pre>`,
        },
      ],
    },
    {
      id: 6,
      questions: [
        {
          question: "Проблема зникання/вибуху градієнтів: Ініціалізація He",
          answer: `<h4>Проблема зникання/вибуху градієнтів та ініціалізація He</h4>
            <p>Проблема зникання градієнтів виникає в глибоких нейронних мережах, коли градієнти стають надзвичайно малими під час зворотного поширення через багато шарів. Це призводить до того, що ранні шари навчаються дуже повільно або взагалі припиняють навчання. Протилежна проблема - вибух градієнтів, коли градієнти стають надто великими та дестабілізують процес навчання.</p>
            
            <p>Ініціалізація He (названа на честь Kaiming He) була розроблена спеціально для вирішення цих проблем при використанні ReLU активацій. Основна ідея полягає в ініціалізації ваг таким чином, щоб дисперсія активацій залишалася постійною через всі шари мережі. Це досягається шляхом встановлення початкових ваг з нормального розподілу зі стандартним відхиленням sqrt(2/n_in), де n_in - кількість вхідних нейронів.</p>
            
            <p>Математично, для He ініціалізації використовується формула: W ~ N(0, sqrt(2/n_in)), що забезпечує оптимальний потік градієнтів через мережу. У Keras це реалізується через kernel_initializer='he_normal' або 'he_uniform'. Ця ініціалізація особливо ефективна для ReLU та її варіантів, оскільки враховує той факт, що ReLU "вбиває" половину нейронів, встановлюючи їх активацію в нуль.</p>`
        },
        {
          question: "Гіперпараметри в глибоких нейронних мережах",
          answer: `<h4>Ключові гіперпараметри глибоких нейронних мереж</h4>
            <p>Гіперпараметри - це параметри моделі, які не навчаються автоматично, а встановлюються до початку тренування та значно впливають на продуктивність мережі. Швидкість навчання (learning rate) є одним з найкритичніших гіперпараметрів, що визначає розмір кроків при оновленні ваг. Занадто висока швидкість може призвести до нестабільності, а занадто низька - до повільного навчання.</p>
            
            <p>Архітектурні гіперпараметри включають кількість прихованих шарів, кількість нейронів у кожному шарі, тип активаційних функцій та метод регуляризації. Розмір пакету (batch size) впливає на стабільність градієнтів та швидкість навчання - великі пакети дають стабільніші градієнти, але потребують більше пам'яті. Кількість епох визначає, скільки разів модель побачить весь датасет під час навчання.</p>
            
            <p>Параметри регуляризації, такі як dropout rate та L1/L2 коефіцієнти, допомагають боротися з перенавчанням. У Keras та sklearn ці параметри можна налаштовувати через відповідні аргументи конструкторів моделей. Сучасні підходи включають використання планувальників швидкості навчання, які адаптивно змінюють learning rate під час тренування. Оптимальний підбір гіперпараметрів часто здійснюється через grid search, random search або більш складні методи оптимізації як Bayesian optimization.</p>`
        },
        {
          question: "Архітектура нейронної мережі з ініціалізацією He",
          answer: `<h4>Практична реалізація мережі з He ініціалізацією</h4>
            <p>Створення глибокої нейронної мережі з He ініціалізацією в Keras потребує явного вказівки kernel_initializer='he_normal' для кожного Dense шару. Ця ініціалізація оптимальна для ReLU активацій та їх варіантів, оскільки враховує специфіку цих функцій активації.</p>
            
            <p>Приклад архітектури:</p>
            <pre><code>
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam

model = Sequential([
    Dense(512, activation='relu', 
          kernel_initializer='he_normal', 
          input_shape=(input_dim,)),
    BatchNormalization(),
    Dropout(0.3),
    
    Dense(256, activation='relu', 
          kernel_initializer='he_normal'),
    BatchNormalization(),
    Dropout(0.3),
    
    Dense(128, activation='relu', 
          kernel_initializer='he_normal'),
    BatchNormalization(),
    Dropout(0.2),
    
    Dense(64, activation='relu', 
          kernel_initializer='he_normal'),
    Dropout(0.2),
    
    Dense(num_classes, activation='softmax',
          kernel_initializer='he_normal')
])

model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])
            </code></pre>
            
            <p>Ця архітектура демонструє правильне використання He ініціалізації разом з батч-нормалізацією та dropout для стабілізації навчання. BatchNormalization допомагає додатково нормалізувати активації між шарами, а Dropout запобігає перенавчанню. Комбінація цих технік з He ініціалізацією забезпечує ефективне навчання глибоких мереж без проблем зникання градієнтів.</p>`
        }
      ]
    },
    {
      id: 7,
      questions: [
        {
          question: "Проблема градієнтів: Ініціалізація Glorot",
          answer: `<h4>Ініціалізація Glorot (Xavier) для стабілізації градієнтів</h4>
            <p>Ініціалізація Glorot, також відома як Xavier ініціалізація, була розроблена для вирішення проблем зникання та вибуху градієнтів у глибоких нейронних мережах. Основна ідея полягає в ініціалізації ваг таким чином, щоб дисперсія вхідних та вихідних сигналів залишалася однаковою через всі шари мережі. Це досягається шляхом встановлення початкових ваг з розподілу, що має дисперсію 2/(n_in + n_out), де n_in та n_out - кількість вхідних та вихідних нейронів відповідно.</p>
            
            <p>Математично, Glorot ініціалізація використовує нормальний розподіл W ~ N(0, sqrt(2/(n_in + n_out))) або рівномірний розподіл U(-sqrt(6/(n_in + n_out)), sqrt(6/(n_in + n_out))). Ця ініціалізація особливо ефективна для активаційних функцій з симетричним розподілом навколо нуля, таких як tanh або sigmoid. На відміну від He ініціалізації, Glorot враховує як вхідні, так і вихідні з'єднання нейрона.</p>
            
            <p>У Keras Glorot ініціалізація реалізується через kernel_initializer='glorot_normal' або 'glorot_uniform'. Вона забезпечує стабільний потік градієнтів та запобігає їх зникання у ранніх шарах мережі. Glorot ініціалізація стала стандартом для багатьох архітектур та часто використовується як початкова точка для експериментів з різними типами активаційних функцій.</p>`
        },
        {
          question: "Переваги та недоліки різних оптимізаторів: RMSProp, Adam, AdaMax, Nadam, AdamW",
          answer: `<h4>Порівняння сучасних оптимізаторів градієнтного спуску</h4>
            <p>RMSProp вирішує проблему зменшення швидкості навчання в AdaGrad шляхом використання експоненціального згладжування для накопичення квадратів градієнтів. Основна перевага - адаптивна швидкість навчання для кожного параметра, що особливо корисно для розріджених градієнтів. Недолік - може бути нестабільним при великих швидкостях навчання та потребує ретельного налаштування гіперпараметрів.</p>
            
            <p>Adam поєднує переваги RMSProp та momentum, використовуючи експоненціальні середні як для градієнтів, так і для їх квадратів. Переваги включають швидку збіжність, низьку чутливість до початкових параметрів та ефективність на розріджених градієнтах. Головний недолік - може погано узагальнювати на деяких задачах через надмірну адаптацію до тренувальних даних.</p>
            
            <p>AdaMax є варіантом Adam, який використовує нескінченну норму замість L2 норми для масштабування швидкості навчання. Nadam додає Nesterov momentum до Adam, що покращує збіжність у випуклих областях. AdamW виправляє проблему weight decay в Adam, правильно реалізуючи L2 регуляризацію. У sklearn та Keras кожен оптимізатор має свої специфічні параметри: Adam потребує налаштування beta1, beta2, а AdamW додатково weight_decay.</p>`
        },
        {
          question: "Архітектура нейронної мережі з ініціалізацією Glorot та оптимізатором Adam",
          answer: `<h4>Практична реалізація мережі з Glorot ініціалізацією та Adam</h4>
            <p>Створення нейронної мережі з Glorot ініціалізацією та оптимізатором Adam демонструє поєднання стабільної ініціалізації ваг із адаптивним оптимізатором. Glorot ініціалізація забезпечує рівномірний розподіл дисперсії через шари, а Adam забезпечує ефективне та стабільне навчання.</p>
            
            <p>Приклад повної архітектури:</p>
            <pre><code>
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

# Створення моделі з Glorot ініціалізацією
model = Sequential([
    Dense(256, activation='tanh', 
          kernel_initializer='glorot_normal',
          kernel_regularizer=l2(0.001),
          input_shape=(input_features,)),
    BatchNormalization(),
    Dropout(0.3),
    
    Dense(128, activation='tanh',
          kernel_initializer='glorot_normal',
          kernel_regularizer=l2(0.001)),
    BatchNormalization(),
    Dropout(0.3),
    
    Dense(64, activation='tanh',
          kernel_initializer='glorot_normal',
          kernel_regularizer=l2(0.001)),
    Dropout(0.2),
    
    Dense(32, activation='tanh',
          kernel_initializer='glorot_normal'),
    
    Dense(output_classes, activation='softmax',
          kernel_initializer='glorot_normal')
])

# Налаштування Adam оптимізатора
adam_optimizer = Adam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-7,
    amsgrad=False
)

# Компіляція моделі
model.compile(
    optimizer=adam_optimizer,
    loss='categorical_crossentropy',
    metrics=['accuracy', 'top_k_categorical_accuracy']
)

# Виведення архітектури
model.summary()
            </code></pre>
            
            <p>Ця архітектура демонструє оптимальне поєднання Glorot ініціалізації з tanh активаціями, що забезпечує стабільний потік градієнтів. Adam оптимізатор налаштований з стандартними параметрами, але може бути адаптований під конкретну задачу. Batch normalization та dropout додають регуляризації, а L2 penalty запобігає перенавчанню ваг.</p>`
        }
      ]
    },
    {
      id: 8,
      questions: [
        {
          question: "Архітектури CNN: LeNet-5",
          answer: `<h4>Архітектура LeNet-5 - піонер згорткових нейронних мереж</h4>
            <p>LeNet-5, розроблена Yann LeCun у 1998 році, стала першою успішною архітектурою згорткових нейронних мереж для розпізнавання рукописних цифр. Архітектура складається з 7 шарів: 2 згорткові шари (C1, C3), 2 шари субдискретизації (S2, S4), 2 повнозв'язних шари (F5, F6) та вихідний шар. Кожен згортковий шар використовує ядра розміром 5x5, а шари субдискретизації застосовують average pooling з розміром 2x2.</p>
            
            <p>Перший згортковий шар C1 має 6 карт ознак розміром 28x28, які отримуються шляхом згортки вхідного зображення 32x32 з 6 ядрами 5x5. Шар субдискретизації S2 зменшує розмір до 14x14 для кожної з 6 карт. Другий згортковий шар C3 містить 16 карт ознак 10x10, з'єднаних не з усіма, а лише з певними картами з попереднього шару для зменшення кількості параметрів. Завершальний шар S4 зменшує розмір до 5x5.</p>
            
            <p>Повнозв'язний шар F5 містить 120 нейронів, F6 - 84 нейрони, а вихідний шар використовує 10 нейронів для класифікації цифр 0-9. LeNet-5 використовує tanh як активаційну функцію та спеціальну функцію втрат на основі Euclidean Radial Basis Function. У сучасних реалізаціях на Keras архітектура адаптується з ReLU активаціями та стандартною categorical crossentropy. Загальна кількість параметрів становить приблизно 60,000, що робить мережу компактною та ефективною для навчання.</p>`
        },
        {
          question: "Навчання нейронних мереж, алгоритм зворотного поширення помилки",
          answer: `<h4>Алгоритм зворотного поширення помилки - основа навчання нейронних мереж</h4>
            <p>Алгоритм зворотного поширення помилки (backpropagation) є основним методом навчання багатошарових нейронних мереж, який дозволяє ефективно обчислювати градієнти функції втрат відносно всіх параметрів мережі. Процес складається з двох фаз: прямого проходу (forward pass), де вхідні дані проходять через мережу для отримання передбачення, та зворотного проходу (backward pass), де помилка поширюється назад для обчислення градієнтів.</p>
            
            <p>Математично алгоритм базується на правилі ланцюга диференціювання, яке дозволяє розкласти складну функцію на композицію простих функцій. Для кожного шару l градієнт обчислюється як добуток локального градієнта активаційної функції та градієнта, що надходить з наступного шару. Формула для оновлення ваг має вигляд: w_new = w_old - α * ∂L/∂w, де α - швидкість навчання, L - функція втрат.</p>
            
            <p>Процес навчання включає ініціалізацію ваг, циклічне виконання прямого та зворотного проходів для кожного батча, обчислення градієнтів та оновлення параметрів за допомогою оптимізатора. У Keras та TensorFlow цей процес автоматизований через автоматичне диференціювання (automatic differentiation). Ключові виклики включають проблеми зникання градієнтів у глибоких мережах, вибір оптимальної швидкості навчання та запобігання перенавчанню через регуляризацію.</p>`
        },
        {
          question: "Реалізація Dropout в Keras",
          answer: `<h4>Практична реалізація Dropout регуляризації в Keras</h4>
            <p>Dropout є потужним методом регуляризації, який запобігає перенавчанню шляхом випадкового відключення частини нейронів під час навчання. В Keras Dropout реалізується через окремий шар, який можна додавати після Dense, Conv2D або інших шарів. Параметр rate визначає частку нейронів, що буде відключена (зазвичай від 0.2 до 0.5).</p>
            
            <p>Базова реалізація Dropout:</p>
            <pre><code>
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten
from tensorflow.keras.layers import BatchNormalization
import numpy as np

# Приклад 1: Dropout у повнозв'язній мережі
model_dense = Sequential([
    Dense(512, activation='relu', input_shape=(784,)),
    Dropout(0.3),  # Відключити 30% нейронів
    
    Dense(256, activation='relu'),
    Dropout(0.4),  # Відключити 40% нейронів
    
    Dense(128, activation='relu'),
    Dropout(0.2),  # Відключити 20% нейронів
    
    Dense(10, activation='softmax')  # Без dropout на вихідному шарі
])

# Приклад 2: Dropout у згортковій мережі
model_cnn = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D((2, 2)),
    Dropout(0.25),  # Dropout після згорткових шарів
    
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Dropout(0.25),
    
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),  # Більший dropout у повнозв'язних шарах
    
    Dense(10, activation='softmax')
])

# Приклад 3: Адаптивний Dropout з різними режимами
from tensorflow.keras.layers import AlphaDropout

model_advanced = Sequential([
    Dense(256, activation='selu', input_shape=(100,)),
    AlphaDropout(0.1),  # Спеціальний dropout для SELU
    
    Dense(128, activation='relu'),
    Dropout(0.3, noise_shape=(None, 1)),  # Структурований dropout
    
    Dense(64, activation='relu'),
    Dropout(0.2, seed=42),  # З фіксованим seed для відтворюваності
    
    Dense(1, activation='sigmoid')
])

# Компіляція з різними налаштуваннями
model_dense.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model_cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
            </code></pre>
            
            <p>Важливо розуміти, що Dropout працює тільки під час навчання та автоматично відключається при inference. У Keras це реалізовано через параметр training, який автоматично керується методами fit() та predict(). Для оптимальних результатів зазвичай використовують менші значення dropout (0.1-0.3) після згорткових шарів та більші (0.4-0.5) у повнозв'язних шарах, ніколи не застосовуючи dropout до вихідного шару.</p>`
        }
      ]
    },
    {
      id: 9,
      questions: [
        {
          question: "Проблема зникання/вибуху градієнтів: Пакетна нормалізація",
          answer: `<h4>Пакетна нормалізація як рішення проблем градієнтів</h4>
            <p>Пакетна нормалізація (Batch Normalization) була запропонована Sergey Ioffe та Christian Szegedy у 2015 році для вирішення проблем зникання та вибуху градієнтів у глибоких нейронних мережах. Основна ідея полягає в нормалізації входів кожного шару таким чином, щоб вони мали нульове середнє та одиничну дисперсію для кожного батча. Це стабілізує процес навчання та дозволяє використовувати вищі швидкості навчання без ризику нестабільності.</p>
            
            <p>Математично пакетна нормалізація працює за формулою: BN(x) = γ * (x - μ) / σ + β, де μ та σ - середнє та стандартне відхилення батча, а γ та β - навчальні параметри масштабування та зсуву. Під час навчання статистики обчислюються для поточного батча, а під час inference використовуються накопичені експоненціальні середні. Це забезпечує консистентність поведінки мережі незалежно від розміру батча при тестуванні.</p>
            
            <p>Пакетна нормалізація вирішує проблему внутрішнього коваріаційного зсуву (internal covariate shift), коли розподіл входів кожного шару змінюється під час навчання через оновлення параметрів попередніх шарів. У Keras BatchNormalization зазвичай розміщується після лінійного перетворення, але перед активаційною функцією. Це дозволяє значно прискорити збіжність, зменшити залежність від ініціалізації ваг та часто діє як додатковий метод регуляризації, зменшуючи потребу в Dropout.</p>`
        },
        {
          question: "Уникнення перенавчання через відключення (Dropout)",
          answer: `<h4>Dropout як метод боротьби з перенавчанням</h4>
            <p>Dropout є одним з найефективніших методів регуляризації для запобігання перенавчанню в нейронних мережах, запропонованим Geoffrey Hinton у 2012 році. Основний принцип полягає у випадковому відключенні частини нейронів під час навчання з імовірністю p (зазвичай 0.2-0.5), що змушує мережу не покладатися на конкретні нейрони та розвивати більш робастні внутрішні представлення. Під час inference всі нейрони активні, але їх виходи масштабуються відповідно до ймовірності dropout.</p>
            
            <p>Механізм дії Dropout базується на ансамблевому ефекті - під час навчання кожен батч фактично тренує дещо відмінну архітектуру мережі через різні комбінації активних нейронів. Це зменшує коадаптацію нейронів, коли вони занадто сильно залежать один від одного, та покращує здатність моделі до узагальнення на нових даних. Dropout особливо ефективний у повнозв'язних шарах, де кількість параметрів велика та ризик перенавчання найвищий.</p>
            
            <p>Оптимальні значення dropout rate залежать від архітектури мережі: для згорткових шарів зазвичай використовують 0.1-0.3, для повнозв'язних шарів 0.3-0.5, а для вихідного шару dropout не застосовується. У sklearn метод аналогічний до dropout не реалізований напряму, але подібний ефект досягається через техніки ensemble та bootstrap aggregating. Важливо розуміти, що занадто високий dropout може призвести до недонавчання, а занадто низький - не забезпечить достатньої регуляризації.</p>`
        },
        {
          question: "Реалізація пакетної нормалізації в Keras",
          answer: `<h4>Практична реалізація BatchNormalization в Keras</h4>
            <p>Пакетна нормалізація в Keras реалізується через шар BatchNormalization, який може бути доданий після будь-якого шару трансформації даних. Оптимальне розміщення - після лінійного перетворення (Dense, Conv2D), але перед активаційною функцією, хоча деякі дослідження показують ефективність розміщення після активації для певних архітектур.</p>
            
            <p>Комплексні приклади реалізації:</p>
            <pre><code>
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten
from tensorflow.keras.layers import BatchNormalization, Dropout, Input
from tensorflow.keras.optimizers import Adam

# Приклад 1: BatchNormalization у повнозв'язній мережі
model_dense = Sequential([
    Dense(512, input_shape=(784,)),
    BatchNormalization(),  # Після Dense, перед активацією
    tf.keras.layers.Activation('relu'),
    Dropout(0.3),
    
    Dense(256),
    BatchNormalization(momentum=0.99, epsilon=1e-5),  # Налаштовані параметри
    tf.keras.layers.Activation('relu'),
    Dropout(0.4),
    
    Dense(128),
    BatchNormalization(),
    tf.keras.layers.Activation('relu'),
    
    Dense(10, activation='softmax')  # Без BN на вихідному шарі
])

# Приклад 2: BatchNormalization у згортковій мережі
input_layer = Input(shape=(28, 28, 1))
x = Conv2D(32, (3, 3))(input_layer)
x = BatchNormalization()(x)
x = tf.keras.layers.Activation('relu')(x)
x = MaxPooling2D((2, 2))(x)

x = Conv2D(64, (3, 3))(x)
x = BatchNormalization(axis=-1)(x)  # Канальна нормалізація
x = tf.keras.layers.Activation('relu')(x)
x = MaxPooling2D((2, 2))(x)
x = Dropout(0.25)(x)

x = Flatten()(x)
x = Dense(128)(x)
x = BatchNormalization()(x)
x = tf.keras.layers.Activation('relu')(x)
x = Dropout(0.5)(x)

output = Dense(10, activation='softmax')(x)
model_cnn = Model(inputs=input_layer, outputs=output)

# Приклад 3: Різні режими BatchNormalization
model_advanced = Sequential([
    Dense(256, input_shape=(100,)),
    BatchNormalization(
        momentum=0.9,          # Швидкість оновлення середніх
        epsilon=1e-5,          # Для числової стабільності
        center=True,           # Навчати β параметр
        scale=True,            # Навчати γ параметр
        trainable=True         # Чи навчаються параметри BN
    ),
    tf.keras.layers.Activation('relu'),
    
    Dense(128),
    BatchNormalization(renorm=True),  # Batch Renormalization
    tf.keras.layers.Activation('relu'),
    
    Dense(1, activation='sigmoid')
])

# Компіляція з врахуванням BatchNormalization
model_dense.compile(
    optimizer=Adam(learning_rate=0.01),  # Можна використати вищу швидкість
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Виведення архітектури
model_cnn.summary()
            </code></pre>
            
            <p>Ключові особливості реалізації включають правильне налаштування параметрів momentum (зазвичай 0.9-0.99) для експоненціального згладжування статистик та epsilon для числової стабільності. BatchNormalization автоматично переключається між тренувальним та inference режимами через параметр training. Важливо розуміти, що BN додає додаткові параметри (γ, β) та обчислювальні витрати, але зазвичай дозволяє використовувати вищі швидкості навчання та менше epochs для досягнення збіжності.</p>`
        }
      ]
    },
    {
      id: 10,
      questions: [
        {
          question: "Проблема зникання/вибуху градієнтів: відсікання градієнта",
          answer: `<h4>Відсікання градієнта як метод стабілізації навчання</h4>
            <p>Відсікання градієнта (gradient clipping) є простим, але ефективним методом боротьби з проблемою вибуху градієнтів у глибоких нейронних мережах. Проблема вибуху градієнтів виникає, коли градієнти стають надто великими під час зворотного поширення, що призводить до нестабільних оновлень ваг та розбіжності процесу навчання. Відсікання градієнта обмежує норму градієнтів до певного порогового значення, запобігаючи надто великим крокам оптимізації.</p>
            
            <p>Існують два основні типи відсікання градієнта: відсікання за значенням (value clipping) та відсікання за нормою (norm clipping). Відсікання за значенням обмежує кожен компонент градієнта до діапазону [-threshold, threshold], тоді як відсікання за нормою масштабує весь вектор градієнта, якщо його норма перевищує поріг. Математично, norm clipping виконується за формулою: g' = g * min(1, threshold/||g||), де g - оригінальний градієнт, g' - відсічений градієнт.</p>
            
            <p>У TensorFlow та Keras відсікання градієнта реалізується через параметр clipvalue або clipnorm в оптимізаторах. Типові значення порогу знаходяться в діапазоні 0.5-5.0 для norm clipping та 0.1-1.0 для value clipping. Відсікання градієнта особливо корисне для рекурентних нейронних мереж (RNN, LSTM), де проблема вибуху градієнтів виникає через повторне множення ваг протягом багатьох часових кроків. Цей метод дозволяє використовувати вищі швидкості навчання без ризику нестабільності та часто покращує збіжність моделі.</p>`
        },
        {
          question: "Уникнення перенавчання через регуляризації L1 і L2",
          answer: `<h4>L1 та L2 регуляризація для боротьби з перенавчанням</h4>
            <p>Регуляризація L1 та L2 є фундаментальними методами запобігання перенавчанню через додавання штрафних термів до функції втрат, які обмежують складність моделі. L2 регуляризація (Ridge) додає до функції втрат термін λ * Σ(w²), де λ - коефіцієнт регуляризації, а w - ваги моделі. Цей метод зменшує великі ваги, роблячи модель більш стабільною та менш чутливою до шуму в даних, але не призводить до точно нульових ваг.</p>
            
            <p>L1 регуляризація (Lasso) використовує штрафний терм λ * Σ|w|, який має тенденцію приводити деякі ваги до точно нульових значень, ефективно виконуючи автоматичний відбір ознак. Це робить L1 регуляризацію особливо корисною для розріджених моделей та інтерпретації, оскільки нульові ваги означають, що відповідні ознаки не впливають на передбачення. Комбінація L1 та L2 регуляризації називається Elastic Net та поєднує переваги обох методів.</p>
            
            <p>Вибір коефіцієнта регуляризації λ є критичним: занадто малі значення не забезпечують достатньої регуляризації, тоді як занадто великі можуть призвести до недонавчання. Типові значення знаходяться в діапазоні 10⁻⁶ до 10⁻¹ та зазвичай підбираються через крос-валідацію. У sklearn регуляризація L1/L2 вбудована в алгоритми як LogisticRegression, LinearRegression з параметром penalty, а в Keras реалізується через kernel_regularizer, bias_regularizer та activity_regularizer в шарах моделі.</p>`
        },
        {
          question: "Реалізація L1 і L2 регуляризації в Keras",
          answer: `<h4>Практична реалізація L1 та L2 регуляризації в Keras</h4>
            <p>Keras надає гнучкі механізми для реалізації L1 та L2 регуляризації через модуль tensorflow.keras.regularizers, який дозволяє застосовувати штрафи до ваг, біасів та активацій нейронів. Регуляризація може бути застосована до kernel (ваги), bias (зміщення) та activity (активації) окремо або в комбінації для досягнення оптимального балансу між точністю та узагальнюючою здатністю моделі.</p>
            
            <p>Детальні приклади реалізації:</p>
            <pre><code>
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Input
from tensorflow.keras.regularizers import l1, l2, l1_l2
from tensorflow.keras.optimizers import Adam
import numpy as np

# Приклад 1: Основні типи регуляризації
model_basic = Sequential([
    # L2 регуляризація тільки для ваг
    Dense(128, activation='relu', 
          kernel_regularizer=l2(0.01),
          input_shape=(784,)),
    
    # L1 регуляризація для ваг та біасів
    Dense(64, activation='relu',
          kernel_regularizer=l1(0.01),
          bias_regularizer=l1(0.01)),
    
    # Комбінована L1+L2 (Elastic Net)
    Dense(32, activation='relu',
          kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),
    
    # Регуляризація активацій
    Dense(16, activation='relu',
          activity_regularizer=l2(0.01)),
    
    Dense(10, activation='softmax')
])

# Приклад 2: Регуляризація у згортковій мережі
input_layer = Input(shape=(28, 28, 1))

# Згорткові шари з L2 регуляризацією
x = Conv2D(32, (3, 3), activation='relu',
           kernel_regularizer=l2(0.001))(input_layer)
x = Conv2D(64, (3, 3), activation='relu',
           kernel_regularizer=l2(0.001))(x)
x = Flatten()(x)

# Повнозв'язні шари з сильнішою регуляризацією
x = Dense(128, activation='relu',
          kernel_regularizer=l1_l2(l1=0.01, l2=0.01),
          bias_regularizer=l2(0.01))(x)
x = Dense(64, activation='relu',
          kernel_regularizer=l2(0.01))(x)

output = Dense(10, activation='softmax')(x)
model_cnn = Model(inputs=input_layer, outputs=output)

# Приклад 3: Адаптивна регуляризація з custom regularizer
def custom_regularizer(weight_matrix):
    return 0.01 * tf.reduce_sum(tf.square(weight_matrix)) + \
           0.005 * tf.reduce_sum(tf.abs(weight_matrix))

model_custom = Sequential([
    Dense(256, activation='relu',
          kernel_regularizer=custom_regularizer,
          input_shape=(100,)),
    
    Dense(128, activation='relu',
          kernel_regularizer=lambda x: 0.01 * tf.nn.l2_loss(x)),
    
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Приклад 4: Різні коефіцієнти для різних шарів
regularization_schedule = [0.001, 0.01, 0.1]  # Збільшення регуляризації

model_scheduled = Sequential()
model_scheduled.add(Dense(512, activation='relu', input_shape=(784,)))

for i, reg_coef in enumerate(regularization_schedule):
    model_scheduled.add(Dense(256 // (i+1), activation='relu',
                             kernel_regularizer=l2(reg_coef),
                             name=f'dense_{i+1}'))

model_scheduled.add(Dense(10, activation='softmax'))

# Компіляція з врахуванням регуляризації
model_basic.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Моніторинг втрат регуляризації
def regularization_loss_callback():
    class RegLossCallback(tf.keras.callbacks.Callback):
        def on_epoch_end(self, epoch, logs=None):
            total_loss = logs.get('loss', 0)
            # Обчислення regularization loss
            reg_loss = sum(model_basic.losses)
            data_loss = total_loss - reg_loss
            print(f"Epoch {epoch}: Data loss: {data_loss:.4f}, Reg loss: {reg_loss:.4f}")
    return RegLossCallback()

# Виведення інформації про регуляризацію
print("Regularization losses:", model_basic.losses)
model_basic.summary()
            </code></pre>
            
            <p>Ключові аспекти реалізації включають правильний вибір коефіцієнтів регуляризації: зазвичай 0.001-0.01 для kernel_regularizer, менші значення (0.0001-0.001) для bias_regularizer. Важливо розуміти, що регуляризація додається до загальної функції втрат, тому треба моніторити як основні втрати, так і регуляризаційні. Комбінування різних типів регуляризації (L1+L2+Dropout+BatchNormalization) часто дає найкращі результати, але потребує ретельного налаштування гіперпараметрів через крос-валідацію.</p>`
        }
      ]
    }
  ],
};
