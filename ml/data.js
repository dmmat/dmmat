const examData = {
  tickets: [
    {
      id: 1,
      questions: [
        {
          question: "Функції активації в глибоких нейронних мережах",
          answer: `<h4>Функції активації в глибоких нейронних мережах</h4>
          <p>
          Функції активації є ключовим компонентом нейронних мереж, що визначають вихідні значення нейронів на основі їх вхідних сигналів. У глибоких мережах найпопулярнішою є функція ReLU (Rectified Linear Unit), яка повертає максимум між нулем та вхідним значенням, що допомагає уникнути проблеми зникнення градієнта. Функція Sigmoid застосовується рідше через насичення при великих значеннях, але корисна для вихідних шарів бінарної класифікації. Функція Tanh має аналогічну до Sigmoid форму, але центрована навколо нуля, що робить її більш підходящою для прихованих шарів. Для більш складних задач використовуються сучасні варіанти як Leaky ReLU, ELU та Swish, які мають кращі властивості градієнта. У Keras функції активації задаються параметром activation='relu' або як окремі шари.
          </p>`,
        },
        {
          question: "Регуляризація на основі Max-норми",
          answer: `<h4>Регуляризація на основі Max-норми</h4>
          <p>
          Max-норма регуляризація є технікою запобігання перенавчанню, яка обмежує максимальне значення ваг нейронної мережі. На відміну від L1 та L2 регуляризації, max-норма встановлює жорсткий ліміт на норму вектора ваг кожного нейрона, зазвичай обмежуючи її значенням від 2 до 4. Цей метод особливо ефективний у поєднанні з dropout, оскільки допомагає стабілізувати навчання після випадкового вимкнення нейронів. В Keras max-норма реалізується через constraint=keras.constraints.MaxNorm(max_value=2.0) при визначенні шарів. Основна перевага цього підходу полягає в тому, що він не додає додаткових членів до функції втрат, а просто обрізає ваги після кожного оновлення. Max-норма регуляризація демонструє особливо хороші результати в рекурентних нейронних мережах та при роботі з нестабільними градієнтами.
          </p>`,
        },
        {
          question: "Задача. Написати архітектуру простої нейронної мережі використовуючи функцію активації ReLU",
          answer: `<h4>Архітектура нейронної мережі з ReLU</h4>
          <p>
          Для створення простої нейронної мережі з функцією активації ReLU використовуємо Sequential API в Keras. Мережа складається з вхідного шару Dense з 128 нейронами та активацією ReLU, прихованого шару з 64 нейронами та тією ж активацією, і вихідного шару з кількістю нейронів відповідно до задачі класифікації. Функція ReLU застосовується до всіх прихованих шарів для забезпечення нелінійності та уникнення проблеми зникнення градієнта. Компіляція моделі включає оптимізатор Adam, функцію втрат sparse_categorical_crossentropy для багатокласової класифікації та метрику accuracy для оцінки продуктивності. Ця архітектура є базовою та може бути адаптована під конкретні задачі шляхом зміни кількості нейронів, шарів та параметрів навчання.
          </p>
          <pre><code>
import tensorflow as tf
from tensorflow import keras

model = keras.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(10, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()
          </code></pre>`,
        },
      ],
    },
    {
      id: 2,
      questions: [
        {
          question: "Різниця між класичним машинним навчанням та глибоким навчанням",
          answer: `<h4>Різниця між класичним машинним навчанням та глибоким навчанням</h4>
          <p>
          Класичне машинне навчання вимагає ручного виділення ознак (feature engineering), де експерти предметної області визначають які характеристики даних є важливими для навчання моделі. Глибоке навчання автоматично виявляє складні патерни та ієрархічні представлення даних через багатошарові нейронні мережі без необхідності попереднього виділення ознак. Класичні алгоритми як SVM, Random Forest та логістична регресія зазвичай працюють з табличними даними та потребують менше обчислювальних ресурсів. Глибокі мережі особливо ефективні для неструктурованих даних як зображення, текст та аудіо, але вимагають значних обчислювальних потужностей та великих обсягів даних. У sklearn реалізовані класичні алгоритми, тоді як TensorFlow/Keras призначені для глибокого навчання. Класичні методи часто більш інтерпретовані та швидші у навчанні на малих датасетах, проте глибоке навчання показує кращі результати на складних задачах з великими обсягами даних.
          </p>`,
        },
        {
          question: "Навчання глибоких нейронних мереж: проблема зникнення градієнта і її вирішення",
          answer: `<h4>Проблема зникнення градієнта та її вирішення</h4>
          <p>
          Проблема зникнення градієнта виникає в глибоких мережах, коли градієнти стають експоненційно малими при поширенні від вихідних до вхідних шарів під час зворотного поширення помилки. Це призводить до повільного або відсутнього навчання початкових шарів мережі, особливо критично для мереж з функціями активації sigmoid або tanh. Основними рішеннями є використання функції активації ReLU та її варіантів, які мають сталі градієнти для додатних значень. Техніки правильної ініціалізації ваг, такі як ініціалізація Glorot (Xavier) або He, допомагають підтримувати стабільні градієнти на початку навчання. Batch normalization нормалізує входи кожного шару, стабілізуючи навчання та дозволяючи використовувати вищі швидкості навчання. Residual connections (skip connections) в архітектурах типу ResNet дозволяють градієнтам протікати напряму через мережу, ефективно вирішуючи проблему зникнення градієнта в дуже глибоких мережах.
          </p>`,
        },
        {
          question: "Задача. Поясніть частину коду conv = keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\")",
          answer: `<h4>Пояснення згорткового шару Conv2D</h4>
          <p>
          Даний код створює двовимірний згортковий шар в Keras, який є основним компонентом згорткових нейронних мереж для обробки зображень. Параметр filters=32 означає, що шар містить 32 фільтри (ядра згортки), кожен з яких виявляє певні ознаки у вхідних даних та створює 32 карти ознак. Kernel_size=3 визначає розмір фільтрів як 3x3 пікселі, що є стандартним розміром для виявлення локальних патернів у зображеннях. Strides=1 означає, що фільтр зсувається на один піксель за раз по горизонталі та вертикалі під час згортки. Параметр padding="same" додає нулі по краях вхідного зображення так, щоб розмір вихідної карти ознак збігався з розміром входу. Activation="relu" застосовує функцію ReLU до результату згортки, забезпечуючи нелінійність та допомагаючи мережі вивчати складні патерни. Цей шар зазвичай використовується на початку згорткових мереж для виявлення базових ознак як краї та текстури.
          </p>`,
        },
      ],
    },
    {
      id: 3,
      questions: [
        {
          question: "Проблема зникання/вибуху градієнтів: Ініціалізація Glorot",
          answer: `<h4>Ініціалізація Glorot для вирішення проблем градієнтів</h4>
          <p>
          Ініціалізація Glorot (також відома як Xavier ініціалізація) розроблена для вирішення проблем зникання та вибуху градієнтів в глибоких нейронних мережах шляхом правильного налаштування початкових ваг. Метод базується на принципі збереження дисперсії сигналів та градієнтів при проходженні через шари мережі. Ініціалізація Glorot встановлює ваги випадковими значеннями з нормального або рівномірного розподілу з дисперсією, що залежить від кількості вхідних та вихідних з'єднань нейрона. Для нормального розподілу дисперсія дорівнює 2/(n_in + n_out), де n_in та n_out - кількість вхідних та вихідних з'єднань відповідно. У Keras ця ініціалізація доступна як kernel_initializer='glorot_uniform' або 'glorot_normal' та є стандартною для багатьох типів шарів. Glorot ініціалізація особливо ефективна з функціями активації sigmoid та tanh, забезпечуючи стабільне навчання глибоких мереж без значних коливань градієнтів.
          </p>`,
        },
        {
          question: "Переваги та недоліки різних оптимізаторів: Моментний оптимізатор, Прискорений градієнт Nesterov, AdaGrad",
          answer: `<h4>Порівняння оптимізаторів: Momentum, Nesterov, AdaGrad</h4>
          <p>
          Моментний оптимізатор (Momentum) додає "інерцію" до градієнтного спуску, накопичуючи експоненційно зважену середню попередніх градієнтів, що допомагає проходити локальні мінімуми та прискорює збіжність в напрямках з послідовними градієнтами. Прискорений градієнт Nesterov покращує Momentum, обчислюючи градієнт не в поточній точці, а в "заглянутій вперед" позиції, що забезпечує більш точне передбачення та швидшу збіжність. AdaGrad адаптує швидкість навчання індивідуально для кожного параметра на основі історії градієнтів, зменшуючи швидкість для часто оновлюваних параметрів та збільшуючи для рідкісних. Основний недолік AdaGrad полягає в монотонному зменшенні швидкості навчання, що може призвести до передчасної зупинки навчання. Momentum та Nesterov можуть "перестрибувати" оптимум через накопичену інерцію, особливо при неправильному налаштуванні гіперпараметрів. У Keras ці оптимізатори реалізовані як keras.optimizers.SGD(momentum=0.9), keras.optimizers.SGD(momentum=0.9, nesterov=True) та keras.optimizers.Adagrad() відповідно.
          </p>`,
        },
        {
          question: "Задача. Написати архітектуру нейронної мережі використовуючи ініціалізацію Glorot та оптимізатор AdaGrad",
          answer: `<h4>Нейронна мережа з ініціалізацією Glorot та оптимізатором AdaGrad</h4>
          <p>
          Створюємо нейронну мережу з явним використанням ініціалізації Glorot для всіх Dense шарів та оптимізатора AdaGrad для навчання. Ініціалізація Glorot uniform застосовується через параметр kernel_initializer, що забезпечує стабільні градієнти на початку навчання. Мережа складається з двох прихованих шарів з ReLU активацією та вихідного шару з softmax для класифікації. Оптимізатор AdaGrad налаштований з початковою швидкістю навчання 0.01, яка автоматично адаптується для кожного параметра на основі історії градієнтів. Така конфігурація особливо корисна для задач з розрідженими даними або нерівномірним розподілом ознак. Модель компілюється з функцією втрат categorical_crossentropy для багатокласової класифікації та метрикою accuracy для оцінки продуктивності. Ця архітектура демонструє практичне застосування теоретичних концепцій ініціалізації ваг та адаптивної оптимізації.
          </p>
          <pre><code>
import tensorflow as tf
from tensorflow import keras

model = keras.Sequential([
    keras.layers.Dense(128, 
                      activation='relu', 
                      kernel_initializer='glorot_uniform',
                      input_shape=(784,)),
    keras.layers.Dense(64, 
                      activation='relu',
                      kernel_initializer='glorot_uniform'),
    keras.layers.Dense(10, 
                      activation='softmax',
                      kernel_initializer='glorot_uniform')
])

optimizer = keras.optimizers.Adagrad(learning_rate=0.01)

model.compile(
    optimizer=optimizer,
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()
          </code></pre>`,
        },
      ],
    },
    {
      id: 4,
      questions: [
        {
          question: "Навчання глибоких нейронних мереж: перенесене навчання",
          answer: `<h4>Перенесене навчання в глибоких нейронних мережах</h4>
          <p>
          Перенесене навчання (Transfer Learning) є потужною технікою, яка дозволяє використовувати попередньо навчені моделі на нових задачах з обмеженою кількістю даних. Основна ідея полягає в тому, що нижні шари глибоких мереж вивчають загальні ознаки (краї, форми, текстури), які корисні для багатьох задач комп'ютерного зору. Зазвичай використовують попередньо навчені моделі на великих датасетах як ImageNet, заморожуючи ваги базової мережі та дотренировуючи лише верхні шари на цільовій задачі. Це значно скорочує час навчання та вимоги до обчислювальних ресурсів порівняно з навчанням з нуля. У Keras перенесене навчання реалізується через keras.applications, де доступні популярні архітектури як VGG, ResNet, InceptionV3 з попередньо навченими вагами. Метод особливо ефективний, коли цільовий датасет схожий на вихідний або коли доступна обмежена кількість навчальних даних, демонструючи значні покращення у точності та швидкості збіжності.
          </p>`,
        },
        {
          question: "Що таке планування швидкості навчання та як воно використовується?",
          answer: `<h4>Планування швидкості навчання</h4>
          <p>
          Планування швидкості навчання (Learning Rate Scheduling) є технікою динамічного регулювання швидкості навчання під час тренування нейронної мережі для покращення збіжності та якості навчання. Початково висока швидкість навчання дозволяє швидко наближатися до оптимуму, а поступове зменшення допомагає точніше налаштувати ваги та уникнути коливань навколо мінімуму. Популярними стратегіями є експоненційне зменшення, ступінчасте зменшення через фіксовані інтервали епох та косинусне загасання. У Keras планування реалізується через keras.callbacks.LearningRateScheduler або keras.callbacks.ReduceLROnPlateau, який автоматично зменшує швидкість при зупинці покращення метрики. Адаптивне планування на основі валідаційної втрати особливо корисне, оскільки реагує на фактичний прогрес навчання. Правильне планування швидкості навчання може значно покращити фінальну точність моделі та забезпечити більш стабільний процес навчання без ручного налаштування гіперпараметрів.
          </p>`,
        },
        {
          question: "Задача. Написати архітектуру нейронної мережі використовуючи планування швидкості навчання",
          answer: `<h4>Нейронна мережа з плануванням швидкості навчання</h4>
          <p>
          Створюємо архітектуру нейронної мережі з реалізацією планування швидкості навчання через callback функції Keras. Модель складається з трьох Dense шарів з dropout для регуляризації та ReLU активацією в прихованих шарах. Використовуємо ReduceLROnPlateau callback, який автоматично зменшує швидкість навчання в 2 рази, якщо валідаційна втрата не покращується протягом 3 епох. Цей підхід дозволяє моделі адаптуватися до складності задачі та уникати застрягання в локальних мінімумах. Початкова швидкість навчання встановлена на 0.001 в оптимізаторі Adam, а callback забезпечує динамічне регулювання під час навчання. Мінімальна швидкість навчання обмежена значенням 0.0001 для запобігання повної зупинки навчання. Така конфігурація забезпечує оптимальний баланс між швидкістю збіжності та стабільністю навчального процесу.
          </p>
          <pre><code>
import tensorflow as tf
from tensorflow import keras

model = keras.Sequential([
    keras.layers.Dense(256, activation='relu', input_shape=(784,)),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(10, activation='softmax')
])

model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

lr_scheduler = keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    min_lr=0.0001,
    verbose=1
)

# Використання при навчанні:
# model.fit(x_train, y_train, validation_data=(x_val, y_val), 
#           epochs=50, callbacks=[lr_scheduler])
          </code></pre>`,
        },
      ],
    },
    {
      id: 5,
      questions: [
        {
          question: "Використання попередньо навчених моделей",
          answer: `<h4>Використання попередньо навчених моделей</h4>
          <p>
          Попередньо навчені моделі є готовими нейронними мережами, які вже навчені на великих датасетах і можуть бути адаптовані для нових задач без навчання з нуля. У Keras доступні популярні архітектури через keras.applications, включаючи VGG16, ResNet50, InceptionV3, MobileNet та EfficientNet, навчені на ImageNet з мільйонами зображень. Основні переваги включають значне скорочення часу навчання, менші вимоги до обчислювальних ресурсів та можливість досягнення високої точності навіть з невеликими датасетами. Типовий підхід полягає в завантаженні базової моделі з weights='imagenet', заморожуванні її ваг через trainable=False, та додаванні власних класифікаційних шарів зверху. Для fine-tuning можна розморозити останні кілька шарів базової моделі і дотренувати їх з малою швидкістю навчання на специфічних даних. Цей метод особливо ефективний для задач комп'ютерного зору, обробки природної мови та інших областей, де доступні якісні попередньо навчені моделі.
          </p>`,
        },
        {
          question: "Що таке об'єднавчі (pooling) шари в згорткових нейронних мережах?",
          answer: `<h4>Об'єднавчі (pooling) шари в згорткових мережах</h4>
          <p>
          Об'єднавчі шари є важливим компонентом згорткових нейронних мереж, призначені для зменшення просторових розмірів карт ознак та зниження обчислювальної складності. Max pooling вибирає максимальне значення з кожного локального регіону карти ознак, зберігаючи найбільш виражені ознаки та забезпечуючи інваріантність до невеликих зсувів у вхідних даних. Average pooling обчислює середнє значення в кожному регіоні, що дає більш згладжене представлення ознак і може бути корисним для зменшення шуму. Зазвичай використовують розмір pooling 2x2 з stride=2, що зменшує розміри карт ознак удвічі по кожному виміру, значно скорочуючи кількість параметрів у наступних шарах. У Keras pooling шари реалізовані як keras.layers.MaxPooling2D() та keras.layers.AveragePooling2D() з можливістю налаштування розміру вікна та кроку. Global pooling варіанти як GlobalMaxPooling2D згортають всю карту ознак до одного значення, що часто використовується перед фінальними класифікаційними шарами замість Flatten.
          </p>`,
        },
        {
          question: "Задача. Написати архітектуру згорткової нейронної мережі використовуючи об'єднавчі шари",
          answer: `<h4>Архітектура згорткової мережі з pooling шарами</h4>
          <p>
          Створюємо класичну архітектуру згорткової нейронної мережі з чергуванням згорткових та pooling шарів для ефективної обробки зображень. Мережа починається з двох згорткових шарів з 32 фільтрами та ReLU активацією, після яких іде MaxPooling2D для зменшення просторових розмірів. Другий блок містить 64 фільтри з аналогічною структурою, що дозволяє виявляти більш складні ознаки на зменшених картах. Третій блок з 128 фільтрами забезпечує виявлення високорівневих патернів перед фінальною класифікацією. GlobalAveragePooling2D замінює Flatten для більш компактного представлення ознак та зменшення кількості параметрів у Dense шарі. Dropout з коефіцієнтом 0.5 запобігає перенавчанню, а фінальний Dense шар з softmax активацією виконує класифікацію. Така архітектура демонструє ефективне використання pooling для поступового зменшення просторових розмірів при збільшенні глибини ознак.
          </p>
          <pre><code>
import tensorflow as tf
from tensorflow import keras

model = keras.Sequential([
    # Перший згортковий блок
    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    keras.layers.Conv2D(32, (3, 3), activation='relu'),
    keras.layers.MaxPooling2D(pool_size=(2, 2)),
    
    # Другий згортковий блок
    keras.layers.Conv2D(64, (3, 3), activation='relu'),
    keras.layers.Conv2D(64, (3, 3), activation='relu'),
    keras.layers.MaxPooling2D(pool_size=(2, 2)),
    
    # Третій згортковий блок
    keras.layers.Conv2D(128, (3, 3), activation='relu'),
    keras.layers.Conv2D(128, (3, 3), activation='relu'),
    keras.layers.MaxPooling2D(pool_size=(2, 2)),
    
    # Класифікатор
    keras.layers.GlobalAveragePooling2D(),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(10, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()
          </code></pre>`,
        },
      ],
    },
  ],
};
